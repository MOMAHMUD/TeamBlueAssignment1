---
title: "Classification Algorithm for Supervised Learning - Heart Disease Diagnosis"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "October 12 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Business Understanding

We work at the Peter Munk Cardiac Centre, a world leader in the diagnosis, care and treatment of patients with both simple and complex cardiac and vascular disease. Each year we incur heavy costs in diagnosing cardiovascular disease because the diagnosis process is at times lengthy and complex involving high tech equipment, intrusive procedures and high skill manpower.

At Peter Munk, we would like to develop a new cardiovascular screening tool using machine learning algorithm based on the cardiovascular related historical data. The screening tool will help the doctors to detect the development of heart diesease so that we can administer correct treatment at the earliest. The new screening process will reduce patients wait time and allow administering the correct treatment. It will also reduce operational costs where the savings could be re-allocated to where it needs.

In summary, the goal for developing this machine learning model is to:

* predict heart diesease diagnosis with high level of accuracy based on the patients' body measurements and attributes.
* Identify the top 3 measurements or attributes which could serve as important indicators for the heart disease diagnosis so that funding can be devoted to improve the facilities and apparatus that produce these measurements.

# Data Understanding

## Data Source and Collection

The data are collected from the UCI Machine Learning Repository site (https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names). The datasets are originated from the four heart disease diagnosis databases from the following four locations dated July 1988:

* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.        
* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.        
* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.        
* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.

The four datasets originally consist of 76 attributes. However, all of the experiments referred to a subset of 14 attributes in which the UCI site also provided with another version of the datasets with these 14 attributes. We took the liberty to use the 14-attribute datasets instead for our prediction exercise.


## Data Exploration

### Load Packages
```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
```
### Load Datasets
Now that the packages are loaded,we can load in the four datasets.

```{r, message=FALSE}

data_cleveland <- read.csv(file.choose(),header = TRUE, na.strings = c("NA","","#NA","?"))
data_hungarian <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
data_switzerland <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
data_VA <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
```

The next step will be to merge all the datasets into one.

```{r, message=FALSE}
full_data <- rbind(data_cleveland,data_hungarian,data_switzerland,data_VA)
```

Now lets look at the structure of the data.

```{r, message=FALSE}
str(full_data)
```

We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 920 observations of 14 variables. To make things a bit more explicit since a couple of the variable names aren't 100% illuminating, here's what we've got to deal with:

Variable Name | Description
--------------|-------------------------------------------------------------------------------------------
AGE           | Age in years
SEX           | Sex: (1 = male; 0 = female)
CP            | Chest Pain Type: (1:typical angina; 2:atypical angina; 3:non-anginal pain; 4:asymptomatic)
TRESTBPS      | Resting Blood Pressure (in mm Hg on admission to the hospital)
CHOL          | Serum Cholestoral in mg/dl
FBS           | Fasting Blood Sugar > 120 mg/dl)  (1 = true; 0 = false)
RESTECG       | Resting Electrocardiographic Results (0:normal; 1:having ST-T wave abnormality; 2:showing                          | probable or definite left ventricular hypertrophy) 
THALACH       | Maximum Heart Rate Achieved
EXANG         | Exercise Induced Angina (1 = yes; 0 = no)
OLDPEAK       | ST Depression Induced by Exercise Relative to Rest 
SLOPE         | The Slope of the Peak Exercise ST Segment (1:upsloping; 2:flat; 3:downsloping) 
CA            | Number of Major Vessels (0-3) Colored by Flourosopy
THAL          | 3 = normal; 6 = fixed defect; 7 = reversable defect
TARGET        | Diagnosis of Heart Disease (0: < 50% diameter narrowing; 1: > 50% diameter narrowing)


# Feature Engineering

Upon initial inspection of the data, we found that the data convention for the TARGET attribute is inconsistent among the 4 datasets. Both Switzerland and VA datasets have the heart disease dignosis target variable with the values of 0, 1, 2, 3 and 4 instead of the values of 0 or 1. Here we will make the data format consistent with the data definition described above (i.e 0: < 50% diameter narrowing; 1: > 50% diameter narrowing). So we made the following adjustments on the TARGET attribute:

```{r, message=FALSE}
# original target variable distributions
table(full_data$TARGET)
full_data$TARGET <- ifelse(full_data$TARGET>=1,1,full_data$TARGET)
# final target variable distributions
table(full_data$TARGET)
```

Also we realize that converting the variable SEX into M & F would be more easy to visulaize instead of "1" and "0" respectively, so we replace them.

```{r, message=FALSE,warning=FALSE}
full_data$SEX<-ifelse(full_data$SEX==1,"M","F")
```

If you see the attribute CHOL has int. values and from common knowledge we know that we can divide the cholestrol into ranges i.e Desirable if chol is <200,"Borderline High" is chol>=200 & <=239 and "High" if it is greater than 240. We create a new column for these ranges "CHOLRANGE".

```{r, message=FALSE,warning=FALSE}
full_data$CHOLRANGE[full_data$CHOL < 200] <- 'Desirable'
full_data$CHOLRANGE[full_data$CHOL >=200 & full_data$CHOL <= 239] <- 'Borderline High'
full_data$CHOLRANGE[full_data$CHOL >=240] <- 'High'
```

##Is cholestrol range related to our Target Varibale?
Now that we've taken care of splitting cholestrol into levels, we can plot it against our TARGET to see if it is related to our TARGET.
We will convert the CHOLRANGE into factor and then work with it to get the plot.
```{r, message=FALSE,warning=FALSE}
full_data$CHOLRANGE<-factor(full_data$CHOLRANGE)
attach(full_data)
freq_tbl_cholrange=table(CHOLRANGE)
head(freq_tbl_cholrange)
freq_xtab_cholrange=xtabs(~CHOLRANGE+TARGET)
head(freq_xtab_cholrange)
barplot(freq_tbl_cholrange)
barplot(freq_xtab_cholrange, legend=rownames(freq_xtab_cholrange), ylab="Number of People", xlab="Target", col=c("mistyrose2","mistyrose4","navajowhite3"), beside=T, las=1)
mosaicplot(table(full_data$CHOLRANGE, full_data$TARGET), main='CHOLESTROL RANGE VS TARGET', shade=TRUE)
```

From this we dont really see any relation between the cholestrol range and the TARGET.So we keep exploring other attributes in order to see any relation with the TARGET.

Lets look at C.P w.r.t. our Target Variable.
```{r, message=FALSE,warning=FALSE}
attach(full_data)
freq_tbl_CP=table(CP)
head(freq_tbl_CP)
freq_xtab_CP=xtabs(~CP+TARGET)
head(freq_xtab_CP)
barplot(freq_xtab_CP, legend=rownames(freq_xtab_CP), ylab="Patients", xlab="Target", col=c("mistyrose2","mistyrose4","navajowhite4","lightsteelblue4"), beside=T, las=1)
```

From this plot we can see that patients with CP(Chest paint Type) 4 is more prone to have >50% diameter narrowing(TARGET=1)

# Missing Data

Now we're ready to start exploring missing data and rectifying it through imputation.

##Summary of the data
First let us summarize the data and see how it looks like.
```{r, message=FALSE,warning=FALSE}
summary(full_data)
```

We can see that the attributes('SEX','CP','FBS','RESTECG','EXANG','SLOPE','CA','THAL','TARGET') are all factor variables so we conver them into factors

```{r, message=FALSE,warning=FALSE}
factor_VARS <- c('SEX','CP','FBS','RESTECG','EXANG','SLOPE','CA','THAL','TARGET')
full_data[factor_VARS]<- lapply(full_data[factor_VARS],function(x) as.factor(x))
```

We have successfully converted all categorical variable in our data set to factors.  Analyzing all the variables we found some that have missing values.  There was a couple variables that caught our attention **CA**  with `r round(sum(is.na(full_data$CA)) / nrow(full_data)*100)` % missing data, **SLOPE** with `r round(sum(is.na(full_data$SLOPE)) / nrow(full_data)*100)` % missing data 
and **THAL** with `r round(sum(is.na(full_data$THAL)) / nrow(full_data)*100)` % missing data.  Since these varibales have a high percent of missing data we chose to remove them.

```{r, message=FALSE,warning=FALSE} 
full_data_truncated<-as.data.frame(full_data[, !names(full_data) %in% c("THAL","CA","SLOPE")])
summary(full_data_truncated)
``` 

#lets check for outliers in TRESTBPS,CHOL,THALACH AND OLDPEAK USING BOXPLOTS.
```{r, message=FALSE,warning=FALSE}
boxplot(full_data_truncated$CHOL,ylab='Resting blood pressure',main='Boxplot distribution of Cholestrol')
boxplot(full_data_truncated$TRESTBPS,ylab='Resting blood pressure',main='Boxplot distribution of TRESTBPS')
boxplot(full_data_truncated$THALACH,ylab='Resting blood pressure',main='Boxplot distribution of THALACH')
boxplot(full_data_truncated$OLDPEAK,ylab='ST Depression Induced by Exercise Relative to Rest',main='Boxplot distribution of OLDPEAK')
```
```{r, message=FALSE,warning=FALSE}

#add some graphs to show before distribution


ggpairs(full_data[, !names(full_data)%in% factor_VARS])

ggpairs(full_data[, !names(full_data)%in% factor_VARS])

aggr_plot = aggr(full_data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(full_data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

full_data_truncated<-as.data.frame(full_data[, !names(full_data) %in% c("THAL","CA","SLOPE")])



```
## Impute Missing Values
```{r, message=FALSE,warning=FALSE}
#Impute missing values using KNN algorithm,remove TARGET variable from KNN
knn_input=as.data.frame(full_data_truncated[, !names(full_data_truncated) %in% c("TARGET")])
#Confrim structure hasnot changed except for lost of target variable
str(knn_input)


#Run KNN imputation, use built in scacle = T to rescale all data.  
knnOutput = knnImputation(knn_input, k=7,scale=T)

#Check if all missing values have been imputeted
summary(knnOutput)

aggr_plot = aggr(knnOutput, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(full_data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data after Imputation","Pattern"))

#add target back to imputated data
knnOutput$TARGET <- full_data$TARGET
#Compare before and after correlation
corr1 <- corrgram(full_data_truncated, order=NULL, panel=panel.shade, text.panel=panel.txt,
         main="Correlogram Before Imputation")
corr2 <- corrgram(knnOutput, order=NULL, panel=panel.shade, text.panel=panel.txt,
         main="Correlogram After Imputation")

#Chart to show before and after categorical




```




# Modelling


## Select Modelling Technique

We now have the data ready to predict our target variable (Diagnosis of Heart Disease 0: < 50% diameter narrowing; 1: > 50% diameter narrowing).  We chose to start with a Decision Tree classification model as they are very intuitive and easy to interpret.  A decision tree works by creating a tree like graph based on differenct variable splits.  These splits are made by evaluating the entropy and Information Gain of each variable split of the data. We evaluate all splits to find the best one that reduces the entropy (messiness) and returns the highest infomration gained. This is recursively run until all data is classified or a stopping criteria is met.

## Split into training & test sets
```{r, message=FALSE,warning=FALSE}
set.seed(456292)
#Create index to split data
train_set <- createDataPartition(knnOutput$TARGET, p = 0.8, list = FALSE)
str(train_set)

#Check distribution of target variable in train set and test set
prop.table(table(knnOutput$TARGET[train_set]))
prop.table(table(knnOutput$TARGET[-train_set]))
prop.table(table(knnOutput$TARGET))

#Create train and test data
train_data <- knnOutput[train_set,]
test_data <- knnOutput[-train_set,]
#Allow for reproducable results

```

We split the data into 80% training data and 20% testing data, the data is balanced on the Target variable to ensure it is representative of the initial data set.

## Build Model 
```{r, message=FALSE,warning=FALSE}
#Find Optimal Parameters for Decision Tree Model and reduce bias using cross validation
folds=10

fitControl <- trainControl(method="cv",number=folds)

#Implement Decision Tree model 
DT_model <- train(factor(TARGET)~., data=train_data, method="rpart",tuneLength = 50, 
                   metric = "Accuracy",
                   trControl = fitControl)



```

We implemened K fold cross validation with 10 folds to find our optimal parameters for the desicion tree model.  The optimal parameters for our data was with a complexity parameter of `r DT_model$finalModel$tuneValue`.  By using K fold cross validation we are able to reduce overfitting and selection bias of the model to our training data.  The cross validation splits the data in 10 folds and creates a model using 9 folds as the training set, and the other fold as the test set.  It repeats this process for each different fold and the validation results are combined(average) from all models to create the final model.




## Assess Model

### Accuracy

```{r, message=FALSE,warning=FALSE}
print(DT_model)
# Predict Test results
DT_model.pred <- predict(DT_model, test_data)
# Check Accuracy of Model 
confus <- confusionMatrix(DT_model.pred,factor(test_data$TARGET))

confus
# Plot Decision Tree
fancyRpartPlot(DT_model$finalModel)



```

The Decision tree model we created had a `r round(confus$overall[1]*100,2)` % accuracy.  We were significanly better at predicting if the TARGET value 1: > 50% diameter narrowing, we were correct `r round(confus$byClass[2]*100,2)` % of the time.  One downside to having a high % of predictor TARGET 1, is that also classified a lot of false negatives.  Of test data that was 0: < 50% diameter narrowing, `r 100-round(confus$byClass[[1]]*100,2)` was incorrectly classified as 1: > 50% diameter narrowing.


### Variable importance

Let's look at relative variable importance of each variable

```{r, message=FALSE, warning=FALSE}
# Get importance
importance    <- varImp(DT_model)

ggplot(importance)


```


# Evaluation

Once we have the business context filled out, just have to talk about the output of the model in terms of business context:
* Comment on prediction accuracy and whether the model can be used for diagnosing patients
* The top 3 measurements which could play a contributing factor to the heart disease diagnosis.

# Deployment






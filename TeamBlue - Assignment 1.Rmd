---
title: "Classification Algorithm for Supervised Learning - Heart Disease Diagnosis"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "October 12 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Statement

Hospital “X” intakes severe and moderate risk patients related to cardiovascular disease. Predicting and maintaining cardiovascular disease is often expensive and tedious; involving high tech equipment, intrusive procedures and high skill manpower. Complexity of the overall process leads delay in diagnosis, providing correct treatment and incur additinal costs. 

# Business Statements

Every successful treatment, and path to a positive patient outcome, begins with a correct and timely diagnosis.

Hospital “X” developed a new cardiovascular screening tool using machine learning algorithm based on the cardiovascular related historic data. The screening tool will help intake and emergency service members to detect the low and high risk patients and administer correct treatment at the earliest. The new screening process will reduce patients wait time, will allow to administer correct treatment, will help to allocate proper resources and will reduce operational costs.


# Data Understandingc

## Data Source and Collection

The data are collected from the UCI Machine Learning Repository site (https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names). The datasets are originated from the four heart disease diagnosis databases from the following four locations dated July 1988:

* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.        
* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.        
* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.        
* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.

The four datasets originally consist of 76 attributes. However, all of the experiments referred to a subset of 14 attributes in which the UCI site also provided with another version of the datasets with these 14 attributes. We took the liberty to use the 14-attribute datasets instead for our prediction exercise.

## Data Descriptions
We aggregated the four datasets described above to create a single dataset. The attribute names along with their descriptions are listed in the table below:

Variable Name | Description
--------------|-------------------------------------------------------------------------------------------
AGE           | Age in years
SEX           | Sex: (1 = male; 0 = female)
CP            | Chest Pain Type: (1:typical angina; 2:atypical angina; 3:non-anginal pain; 4:asymptomatic)
TRESTBPS      | Resting Blood Pressure (in mm Hg on admission to the hospital)
CHOL          | Serum Cholestoral in mg/dl
FBS           | Fasting Blood Sugar > 120 mg/dl)  (1 = true; 0 = false)
RESTECG       | Resting Electrocardiographic Results (0:normal; 1:having ST-T wave abnormality; 2:showing                     | probable or definite left ventricular hypertrophy) 
THALACH       | Maximum Heart Rate Achieved
EXANG         | Exercise Induced Angina (1 = yes; 0 = no)
OLDPEAK       | ST Depression Induced by Exercise Relative to Rest 
SLOPE         | The Slope of the Peak Exercise ST Segment (1:upsloping; 2:flat; 3:downsloping) 
CA            | Number of Major Vessels (0-3) Colored by Flourosopy
THAL          | 3 = normal; 6 = fixed defect; 7 = reversable defect
TARGET        | Diagnosis of Heart Disease (0: < 50% diameter narrowing; 1: > 50% diameter narrowing)

## Data Exploration

### Load Packages
```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
```
### Load Datasets
We can now load in the four datasets and then we merge them into one.

```{r, message=FALSE}

data_cleveland <- read.csv(file.choose(),header = TRUE, na.strings = c("NA","","#NA","?"))
data_hungarian <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
data_switzerland <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
data_VA <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA","?"))
full_data <- rbind(data_cleveland,data_hungarian,data_switzerland,data_VA)
```
Upon initial inspection of the data, we found that the data convention for the TARGET attribute is inconsistent among the 4 datasets. Both Switzerland and VA datasets have the heart disease dignosis target variable with the values of 0, 1, 2, 3 and 4 instead of the values of 0 or 1. Here we will make the data format consistent with the data definition described above (i.e 0: < 50% diameter narrowing; 1: > 50% diameter narrowing). So we made the following adjustments on the TARGET attribute:
```{r, message=FALSE}
# original target variable distributions
table(full_data$TARGET)
full_data$TARGET <- ifelse(full_data$TARGET>=1,1,full_data$TARGET)
# final target variable distributions
table(full_data$TARGET)
```
# Data Preparation

## Select Data

```{r, message=FALSE,warning=FALSE}
#Analyze variable types
str(full_data)
#Factor Categorical variables
factor_VARS <- c('SEX','CP','FBS','RESTECG','EXANG','SLOPE','CA','THAL')
full_data[factor_VARS]<- lapply(full_data[factor_VARS],function(x) as.factor(x))

# see distribution and missing values of each variable
summary(full_data)
#add some graphs to show before distribution


ggpairs(full_data[, !names(full_data)%in% factor_VARS])

ggpairs(full_data[, !names(full_data)%in% factor_VARS])

aggr_plot = aggr(full_data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(full_data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

full_data_truncated<-as.data.frame(full_data[, !names(full_data) %in% c("THAL","CA","SLOPE")])



```
After inputing the data, we converted all categorical variable in our data set to factors.  Analyzing all the variables we found some that have missing values.  There was a couple variables that caught our attention **CA**  with `r round(sum(is.na(full_data$CA)) / nrow(full_data)*100)` % missing data, **SLOPE** with `r round(sum(is.na(full_data$SLOPE)) / nrow(full_data)*100)` % missing data 
and **THAL** with `r round(sum(is.na(full_data$THAL)) / nrow(full_data)*100)` % missing data.  Since these varibales have a high percent of missing data we chose to remove them.
## Impute Missing Values
```{r, message=FALSE,warning=FALSE}
#Impute missing values using KNN algorithm,remove TARGET variable from KNN
knn_input=as.data.frame(full_data_truncated[, !names(full_data_truncated) %in% c("TARGET")])
#Confrim structure hasnot changed except for lost of target variable
str(knn_input)


#Run KNN imputation, use built in scacle = T to rescale all data.  
knnOutput = knnImputation(knn_input, k=7,scale=T)

#Check if all missing values have been imputeted
summary(knnOutput)

aggr_plot = aggr(knnOutput, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(full_data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data after Imputation","Pattern"))

#add target back to imputated data
knnOutput$TARGET <- full_data$TARGET
#Compare before and after correlation
corr1 <- corrgram(full_data_truncated, order=NULL, panel=panel.shade, text.panel=panel.txt,
         main="Correlogram Before Imputation")
corr2 <- corrgram(knnOutput, order=NULL, panel=panel.shade, text.panel=panel.txt,
         main="Correlogram After Imputation")

#Chart to show before and after categorical




```




# Modelling


## Select Modelling Technique

We now have the data ready to predict our target variable (Diagnosis of Heart Disease 0: < 50% diameter narrowing; 1: > 50% diameter narrowing).  We chose to start with a Decision Tree classification model as they are very intuitive and easy to interpret.  A decision tree works by creating a tree like graph based on differenct variable splits.  These splits are made by evaluating the entropy and Information Gain of each variable split of the data. We evaluate all splits to find the best one that reduces the entropy (messiness) and returns the highest infomration gained. This is recursively run until all data is classified or a stopping criteria is met.

## Split into training & test sets
```{r, message=FALSE,warning=FALSE}
set.seed(456292)
#Create index to split data
train_set <- createDataPartition(knnOutput$TARGET, p = 0.8, list = FALSE)
str(train_set)

#Check distribution of target variable in train set and test set
prop.table(table(knnOutput$TARGET[train_set]))
prop.table(table(knnOutput$TARGET[-train_set]))
prop.table(table(knnOutput$TARGET))

#Create train and test data
train_data <- knnOutput[train_set,]
test_data <- knnOutput[-train_set,]
#Allow for reproducable results

```

We split the data into 80% training data and 20% testing data, the data is balanced on the Target variable to ensure it is representative of the initial data set.

## Build Model 
```{r, message=FALSE,warning=FALSE}
#Find Optimal Parameters for Decision Tree Model and reduce bias using cross validation
folds=10

fitControl <- trainControl(method="cv",number=folds)

#Implement Decision Tree model 
DT_model <- train(factor(TARGET)~., data=train_data, method="rpart",tuneLength = 50, 
                   metric = "Accuracy",
                   trControl = fitControl)



```

We implemened K fold cross validation with 10 folds to find our optimal parameters for the desicion tree model.  The optimal parameters for our data was with a complexity parameter of `r DT_model$finalModel$tuneValue`.  By using K fold cross validation we are able to reduce overfitting and selection bias of the model to our training data.  The cross validation splits the data in 10 folds and creates a model using 9 folds as the training set, and the other fold as the test set.  It repeats this process for each different fold and the validation results are combined(average) from all models to create the final model.




## Assess Model

### Accuracy

```{r, message=FALSE,warning=FALSE}
print(DT_model)
# Predict Test results
DT_model.pred <- predict(DT_model, test_data)
# Check Accuracy of Model 
confus <- confusionMatrix(DT_model.pred,factor(test_data$TARGET))

confus
# Plot Decision Tree
fancyRpartPlot(DT_model$finalModel)



```

The Decision tree model we created had a `r round(confus$overall[1]*100,2)` % accuracy.  We were significanly better at predicting if the TARGET value 1: > 50% diameter narrowing, we were correct `r round(confus$byClass[2]*100,2)` % of the time.  One downside to having a high % of predictor TARGET 1, is that also classified a lot of false negatives.  Of test data that was 0: < 50% diameter narrowing, `r 100-round(confus$byClass[[1]]*100,2)` was incorrectly classified as 1: > 50% diameter narrowing.


### Variable importance

Let's look at relative variable importance of each variable

```{r, message=FALSE, warning=FALSE}
# Get importance
importance    <- varImp(DT_model)

ggplot(importance)


```


# Evaluation

Once we have the business context filled out, just have to talk about the output of the model in terms of business context





